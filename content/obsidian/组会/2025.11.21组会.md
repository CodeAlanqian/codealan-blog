---



title: 2025.11.21组会
date: 2025-11-21
lastmod: 2025-11-26
draft: false
tags:
- 组会
- Ubuntu
- ROS
- VLN
- Git
- LLM
---




部署到无人机上，会有视点可能不匹配的问题 Gap

现在的VLN还没收敛到可以用很低的成本去实现，也就是真正在边侧上部署的并没有

DDPPO网络预测深度


AerialVLN复现

最后我想简单地做个真机部署，就通过Crazyflie或者tello

同时还调研了一下目前小模型的情况：
- 项目同时开源了大模型的极简结构-包含拓展共享混合专家(MoE)、数据集清洗、预训练(Pretrain)、监督微调(SFT)、LoRA微调、直接偏好优化(DPO)、强化学习训练(RLAIF: PPO/GRPO等)、模型蒸馏等全过程代码。
[jingyaogong/minimind: 🚀🚀 「大模型」2小时完全从0训练26M的小参数GPT！🌏 Train a 26M-parameter GPT from scratch in just 2h!](https://github.com/jingyaogong/minimind)
在线体验：
[MiniMind-Reasoning · 创空间](https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning)


[jingyaogong/minimind-v: 🚀 「大模型」1小时从0训练26M参数的视觉多模态VLM！🌏 Train a 26M-parameter VLM from scratch in just 1 hours!](https://github.com/jingyaogong/minimind-v)
项目同时包含了VLM大模型的极简结构、数据集清洗、预训练(Pretrain)、监督微调(SFT)等全过程代码
在线体验
[MiniMind-V · 创空间](https://www.modelscope.cn/studios/gongjy/MiniMind-V)


---

## 1. AerialVLN 是什么 & 怎么复现

AerialVLN 是 ICCV 2023 提出的空中视觉-语言导航任务，基于 25 个城市级场景、8446 条人工飞行轨迹和 2.5 万条指令构建（在 3D 模拟器里飞 UAV，根据自然语言指令导航）。([arXiv](https://arxiv.org/abs/2308.06735?utm_source=chatgpt.com "AerialVLN: Vision-and-Language Navigation for UAVs"))  
官方代码和数据都挂在 GitHub: `AirVLN/AirVLN`。([GitHub](https://github.com/AirVLN/AirVLN?utm_source=chatgpt.com "AirVLN/AirVLN"))

### 1.1 环境准备（总体）

大致依赖链你可以这样理解：

- OS：推荐 Ubuntu 20.04 / 22.04
    
- Python：3.8–3.10
    
- 深度学习：PyTorch + CUDA
    
- 仿真环境：他们自建的 3D simulator（基于近真实图片的城市场景）
    
- 其他：常规 CV/NLP 库（transformers、opencv-python 等）
    

复现最稳妥的玩法是：

1. 按 GitHub README 里指定的 Python 版本和 requirements 建环境。
    
2. 用 conda 建一个 `aerialvln` 专用 env，把 PyTorch 按你本地显卡匹配的 CUDA 版本装好。
    
3. 再装项目自带的依赖（`pip install -r requirements.txt`）。
    

### 1.2 下载数据 & 场景

AerialVLN 数据集可以从：

- Kaggle 镜像：AerialVLN Dataset([Kaggle](https://www.kaggle.com/datasets/shuboliu/aerialvln?utm_source=chatgpt.com "AerialVLN"))
    
- 或者论文页面给出的链接 / GitHub Release([麦考瑞大学研究者](https://researchers.mq.edu.au/en/publications/aerialvln-vision-and-language-navigation-for-uavs/?utm_source=chatgpt.com "AerialVLN: vision-and-language navigation for UAVs"))
    

一般包含：

- 视觉数据：预渲染好的图像序列（或者渲染参数）
    
- 语言指令：JSON/文本，配对每条轨迹
    
- 轨迹 & 动作序列：用于监督学习和评估
    

按仓库说明把它们放到类似：

```bash
data/
  aerialvln/
    images/
    trajs/
    annotations/
```

再根据 config 文件（一般是 `.yaml` 或 `.json`）里写的路径改成你本地的绝对/相对路径。

### 1.3 运行官方 baseline

AerialVLN 提供了基于 Cross-Modal Alignment (CMA) 的 baseline 模型([麦考瑞大学研究者](https://researchers.mq.edu.au/en/publications/aerialvln-vision-and-language-navigation-for-uavs/?utm_source=chatgpt.com "AerialVLN: vision-and-language navigation for UAVs"))。典型流程：

1. **预处理**：
    
    - 提取图像特征（比如 ResNet / ViT）
        
    - 文本用 BERT / RoBERTa 编码  
        仓库里通常有 `preprocess_xxx.py`，跑完会得到 `.pt` 或 `.npy` 的特征缓存。
        
2. **训练**：  
    类似：
    
    ```bash
    python train.py \
      --config configs/aerialvln_cma.yaml \
      --gpu 0
    ```
    
    你需要注意：
    
    - batch size 和 GPU 显存
        
    - 训练 epoch / 学习率与官方设置保持一致
        
3. **评估**：  
    一般有 `eval.py` 或在 `train.py` 有 `--eval` 模式。  
    评估指标包括成功率、路径偏差等（和传统 VLN 类似，但是是 3D 连续控制）。
    

你的目标是：  
**在官方 test/val split 上接近论文里的数值**，比如 Oracle Success Rate、Success weighted by Path Length 等。([CVF开放获取](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_AerialVLN_Vision-and-Language_Navigation_for_UAVs_ICCV_2023_paper.pdf?utm_source=chatgpt.com "AerialVLN: Vision-and-Language Navigation for UAVs"))

### 1.4 仿真器与“真飞行”无关的小坑

AerialVLN 的仿真环境是 **基于真实城市图像渲染的 3D 模拟器**，但运行时只需要：

- 他们打包好的环境/数据
    
- 按照接口输出离散动作（如上/下/左/右/前/后/停）或连续控制
    

不需要你自己去装 Unreal / AirSim 这类重量级引擎，除非你想做扩展工作。

---

## 2. AirVLN（GitHub 项目）怎么用

`AirVLN/AirVLN` 这个仓库实际上就是 AerialVLN 项目的官方实现和数据入口，它既定义了任务，也提供了基线。([GitHub](https://github.com/AirVLN/AirVLN?utm_source=chatgpt.com "AirVLN/AirVLN"))

你可以把“复现 AirVLN”理解为：

- 跑通这个仓库提供的 baseline 模型
    
- 在其定义的任务设置（如 Single-step / Multi-step / Long trajectory）下拿到接近论文的结果
    
- 如果想更进一步，可以对照最近的改进方法，比如 STMR-VLN、CityNavAgent、OpenFly-Agent 等做对比实验。([The Minimum Viable Model](https://eziotic.github.io/STMR-VLN.github.io/?utm_source=chatgpt.com "Aerial Vision-and-Language Navigation via Semantic-Topo ..."))
    

### 2.1 仓库典型结构（大致）

大致会包含（名字不完全一致，但功能类似）：

- `datasets/`：数据加载器、环境接口
    
- `models/`：Cross-modal encoder、policy network、LLM-based agent 等
    
- `scripts/`：训练/评估脚本
    
- `configs/`：不同模型和超参的配置文件
    

你需要做的事：

1. **确认 data 路径**：  
    config 里一般有 `DATA_ROOT`、`IMG_DIR`、`TRAJ_DIR` 等参数，你改成本地。
    
2. **确认模型 checkpoint 路径**：  
    如果仓库给了预训练模型，就可以直接 `--eval_only` 跑一遍，先 check 环境。
    
3. **训练自己的模型**：  
    完整跑一次训练流程，看最终指标是否接近/复现论文结果。
    

---

## 3. 复现时容易踩的坑（给你提前填上）

结合你以前玩 Crazyflie / SLAM / 返航那一套经验，我猜你会遇到这些点：

1. **版本地狱**
    
    - PyTorch / CUDA 对不上：先按你机器的 CUDA 装 PyTorch，再 `pip install -r requirements`，不然会被重装覆盖。
        
    - transformers / tokenizers 版本变了，造成模型 load 失败：遇到报错就把版本回退到 README 里写的。
        
2. **随机种子**  
    论文通常会固定种子才能得到稳定指标；你复现时至少保证：
    
    - Python、NumPy、PyTorch 的 seed 固定
        
    - 指定 `deterministic` / `cudnn.benchmark` 等参数
        
3. **数据 split 对不上**  
    一定要用他们给的 train/val/test 划分；VLN 这种任务极其依赖 split 一致，不然数字看起来“超越论文”，实际上是在作弊。
    
4. **评估细节**  
    数据集中经常有：
    
    - “成功”的定义：距离目标小于某半径 + 是否满足高度约束
        
    - 路径长度归一化  
        不按他们的 metric 实现，很容易偏差 5–10 个点。
        

---

## 4. 如果你是想做“基于 AerialVLN / AirVLN 的改进研究”

既然你现在本职就是搞 UAV、SLAM、视觉返航，我按“科研升级路线”帮你排个简单 roadmap：

1. **Step 1：纯复现**
    
    - 跑通 GitHub 官方 baseline
        
    - 把表一的核心指标（SR、OSR、SPL 等）复现到 ±1–2% 范围([CVF开放获取](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_AerialVLN_Vision-and-Language_Navigation_for_UAVs_ICCV_2023_paper.pdf?utm_source=chatgpt.com "AerialVLN: Vision-and-Language Navigation for UAVs"))
        
2. **Step 2：读一圈相关工作**
    
    - STMR-VLN：用 LLM + Semantic-Topo-Metric 表示做零样本 aerial VLN([The Minimum Viable Model](https://eziotic.github.io/STMR-VLN.github.io/?utm_source=chatgpt.com "Aerial Vision-and-Language Navigation via Semantic-Topo ..."))
        
    - CityNav / CityNavAgent：城市尺度语言导航，带地理信息的 dataset 和 LLM agent([arXiv](https://arxiv.org/abs/2406.14240?utm_source=chatgpt.com "CityNav: Language-Goal Aerial Navigation Dataset with Geographic Information"))
        
    - OpenFly：更大规模 aerial VLN 平台，支持 Unreal / GTA V / Google Earth / 3DGS 等渲染([shailab-ipec.github.io](https://shailab-ipec.github.io/openfly/?utm_source=chatgpt.com "OpenFly"))
        
3. **Step 3：结合你已有的“视觉返航 + SLAM”技能**  
    一些有潜力、而且和你现在研究高度对齐的方向：
    
    - 用你现有的单目 SLAM / 光流返航模块，在 AerialVLN 里当 **低层运动控制 + 地图构建模块**，上层只做语言到高层路径规划。
        
    - 把 Crazyflie / 自己的 UAV 平台做一个 **小规模 sim-to-real 验证**：  
        在 AerialVLN 环境学到的策略，迁移到你已有的真实 UAV（哪怕只在室外小场景/简化语义指令上）。
        
4. **Step 4：写论文 / 报告的时候**
    
    - 把 AerialVLN 任务形式、数据规模、基线性能完整介绍一遍（相当于 Related Work 中的“空中 VLN 基线”块）。
        
    - 重点强调你方法在“长航程导航”、“复杂高度约束”、“视觉返航鲁棒性”上的优势。
        

---

## 5. 给你一个简化执行 checklist ✅

你现在如果要“开始动手”，可以就照这个：

1. `git clone https://github.com/AirVLN/AirVLN.git`
    
2. 建虚拟环境，装 PyTorch + `pip install -r requirements.txt`
    
3. 下载 AerialVLN 数据集（官方链接或 Kaggle），放到 `data/aerialvln/`
    
4. 修改 config 里的路径（数据、日志、ckpt）
    
5. 跑一次官方给的预训练权重评估脚本，确认环境 OK
    
6. 跑完整训练，记录指标，对照论文表
    
7. 把你复现到的数值、训练日志、环境版本整理成一个 `reproduce.md`，以后写论文/开源都能直接复用
    

---

总之，这两个任务本质上是“一套空中 VLN 基准 + 官方代码仓库”，你要做的是：  
**把代码和数据跑通 → 指标对上 → 在此之上玩你自己的 SLAM / 返航 / LLM trick**。

接下来你想的话，可以告诉我你打算用哪台机器（显卡 / 内存），我可以帮你直接把 conda 与 PyTorch 的安装命令、以及一个最小可运行的训练命令写出来，省点环境调试时间，把精力留给真正有意思的地方：让无人机听人话。